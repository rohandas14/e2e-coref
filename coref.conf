base {
    ########## DATA #########
    language = pl_pcc-corefud
    max_ment_width = 30         # maximal number of words in a single mention
    genres = [bc, bn, mz, nw, pt, tc, wb] # possible genres for a document

    ######### MODEL ########
    bin_widths = [1, 1, 1, 1, 1, 3, 8, 16, 32, 1]   # width of bins for mention distance embedding
    dropout = 0.3               # dropout probability for certain layers
    feature_size = 20           # embedding size for certain features e.g. genre, width etc.
    hidden_size = 3000          # hidden layer size of mention and antecedent scorers
    hidden_depth = 1            # number of hidden layers for several scorers
    max_ment_dist = 250         # maximal distance to antecedent (measured in words)
    ment_ratio = 0.4            # used to calculate top k mention to consider
    max_antes = 50              # max number of antecedents per mention to consider
    coref_depth = 2             # number of iterations of inference procedure

    ####### TRAINING #######
    train_data_file = train.pl_pcc-corefud.jsonlines
    epochs = 60                # number of epochs to fine-tune bert and train task-model

    ###### EVALUATION ######
    eval_gold_path = ./data/data/dev.pl_pcc-corefud.v4_auto_conll
    eval_data_file = dev.pl_pcc-corefud.jsonlines
    # eval_gold_path = ./data/data/dev.english.v4_auto_conll
    # eval_data_file = dev.english.jsonlines
    # eval_gold_path = ./data/data/dev.english.v4_gold_conll
    # eval_data_file = dev.english.jsonlines
}


bert-base = ${base}{
    ######### BERT #########
    bert = bert-base-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 128             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/bert_base/
}


bert-large = ${base}{
    ######### BERT #########
    bert = bert-large-cased     # name of hugging face transformer model
    bert_emb_size = 1024        # name of hugging face transformer model
    segm_size = 384             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3            # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/bert_large
}


spanbert-base = ${base}{
    ######### BERT #########
    bert = SpanBERT/spanbert-base-cased         # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 384             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3            # maximal number of segments per document (applied only for training)
    lr_bert = 2e-05             # initial learning rate for fine-tuning bert
    lr_task = 1e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/spanbert_base
}


spanbert-large = ${base}{
    ######### BERT #########
    bert = SpanBERT/spanbert-large-cased        # name of hugging face transformer model
    bert_emb_size = 1024        # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3            # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 3e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/spanbert_large
}

roberta-base = ${base}{
    ######### BERT #########
    bert = roberta-base         # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-05             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/roberta_base
}

multilingual-bert-base = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_Polish-PCC/
    data_folder = ./data/data/multilingual_bert_base
}

xlm-roberta-base = ${base}{
    ######### BERT #########
    bert = xlm-roberta-base                     # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_Spanish-AnCora/
    data_folder = ./data/data/xlm_roberta_base
}

multilingual-bert-base-russian = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs 

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_Russian-RuCor/
    data_folder = ./data/data/multilingual_bert_base

    ######### OVERRIDE ##########
    language = ru_rucor-corefud
    train_data_file = train.ru_rucor-corefud.jsonlines
    epochs = 300               
    eval_gold_path = ./data/data/dev.ru_rucor-corefud.v4_auto_conll
    eval_gold_coreud_path = ./data/data/ru_rucor-corefud-dev.conllu
    eval_data_file = dev.ru_rucor-corefud.jsonlines
    predictions_path = ./predictions/baseline/russian_baseline_corefud.conllu
}

multilingual-bert-base-polish = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 

    ######## LOGGING ########
    log_after = 100              # log after every 'n' batches

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_Polish-PCC/
    data_folder = ./data/data/multilingual_bert_base

    ######### OVERRIDE ##########
    language = pl_pcc-corefud
    train_data_file = train.pl_pcc-corefud.jsonlines
    epochs = 60               
    eval_gold_path = ./data/data/dev.pl_pcc-corefud.v4_auto_conll
    eval_gold_coreud_path = ./data/data/pl_pcc-corefud-dev.conllu
    eval_data_file = dev.pl_pcc-corefud.jsonlines
    predictions_path = ./predictions/baseline/polish_baseline_corefud.conllu
}

multilingual-bert-base-spanish = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 

    ######## LOGGING ########
    log_after = 100              # log after every 'n' batches

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_Spanish-AnCora/
    data_folder = ./data/data/multilingual_bert_base

    ######### OVERRIDE ##########
    language = es_ancora-corefud
    train_data_file = train.es_ancora-corefud.jsonlines
    epochs = 60               
    eval_gold_path = ./data/data/dev.es_ancora-corefud.v4_auto_conll
    eval_gold_coreud_path = ./data/data/es_ancora-corefud-dev.conllu
    eval_data_file = dev.es_ancora-corefud.jsonlines
    predictions_path = ./predictions/baseline/spanish_corefud.conllu
}

spanish-bert = ${base}{
    ######### BERT #########
    bert = dccuchile/bert-base-spanish-wwm-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 

    ######## LOGGING ########
    log_after = 100              # log after every 'n' batches

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_Spanish-AnCora/
    data_folder = ./data/data/bert-base-spanish

    ######### OVERRIDE ##########
    language = es_ancora-corefud
    train_data_file = train.es_ancora-corefud.jsonlines
    epochs = 60               
    eval_gold_path = ./data/data/dev.es_ancora-corefud.v4_auto_conll
    eval_gold_coreud_path = ./data/data/es_ancora-corefud-dev.conllu
    eval_data_file = dev.es_ancora-corefud.jsonlines
    predictions_path = ./predictions/baseline/spanish_corefud.conllu
}

multilingual-bert-base-catalan = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 

    ######## LOGGING ########
    log_after = 100              # log after every 'n' batches

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_Catalan-AnCora/
    data_folder = ./data/data/multilingual_bert_base

    ######### OVERRIDE ##########
    language = ca_ancora-corefud
    train_data_file = train.ca_ancora-corefud.jsonlines
    epochs = 60               
    eval_gold_path = ./data/data/dev.ca_ancora-corefud.v4_auto_conll
    eval_gold_coreud_path = ./data/data/ca_ancora-corefud-dev.conllu
    eval_data_file = dev.ca_ancora-corefud.jsonlines
    predictions_path = ./predictions/baseline/catalan_corefud.conllu
}

multilingual-bert-base-hungarian = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs 

    ######## LOGGING ########
    log_after = 20              # log after every 'n' batches

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_Hungarian-SzegedKoref/
    data_folder = ./data/data/multilingual_bert_base

    ######### OVERRIDE ##########
    language = hu_szegedkoref-corefud
    train_data_file = train.hu_szegedkoref-corefud.jsonlines
    epochs = 500               
    eval_gold_path = ./data/data/dev.hu_szegedkoref-corefud.v4_auto_conll
    eval_gold_coreud_path = ./data/data/hu_szegedkoref-corefud-dev.conllu
    eval_data_file = dev.hu_szegedkoref-corefud.jsonlines
    predictions_path = ./predictions/baseline/hungarian_corefud.conllu
}

multilingual-bert-base-lithuanian = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs 

    ######## LOGGING ########
    log_after = 20              # log after every 'n' batches

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_Lithuanian-LCC/
    data_folder = ./data/data/multilingual_bert_base

    ######### OVERRIDE ##########
    language = lt_lcc-corefud
    train_data_file = train.lt_lcc-corefud.jsonlines
    epochs = 500               
    eval_gold_path = ./data/data/dev.lt_lcc-corefud.v4_auto_conll
    eval_gold_coreud_path = ./data/data/lt_lcc-corefud-dev.conllu
    eval_data_file = dev.lt_lcc-corefud.jsonlines
    predictions_path = ./predictions/baseline/lithuanian_corefud.conllu
}