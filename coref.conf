base {
    ########## DATA #########
    language = pl_pcc-corefud
    max_ment_width = 30         # maximal number of words in a single mention
    genres = [bc, bn, mz, nw, pt, tc, wb] # possible genres for a document

    ######### MODEL ########
    bin_widths = [1, 1, 1, 1, 1, 3, 8, 16, 32, 1]   # width of bins for mention distance embedding
    dropout = 0.3               # dropout probability for certain layers
    feature_size = 20           # embedding size for certain features e.g. genre, width etc.
    hidden_size = 3000          # hidden layer size of mention and antecedent scorers
    hidden_depth = 1            # number of hidden layers for several scorers
    max_ment_dist = 250         # maximal distance to antecedent (measured in words)
    ment_ratio = 0.4            # used to calculate top k mention to consider
    max_antes = 50              # max number of antecedents per mention to consider
    coref_depth = 2             # number of iterations of inference procedure

    ####### TRAINING #######
    # train_data_file = train.pl_pcc-corefud.jsonlines
    # epochs = 60                # number of epochs to fine-tune bert and train task-model

    ###### EVALUATION ######
    # eval_gold_path = ./data/data/dev.pl_pcc-corefud.v4_auto_conll
    # eval_data_file = dev.pl_pcc-corefud.jsonlines
    # eval_gold_path = ./data/data/dev.english.v4_auto_conll
    # eval_data_file = dev.english.jsonlines
    # eval_gold_path = ./data/data/dev.english.v4_gold_conll
    # eval_data_file = dev.english.jsonlines
}


bert-base = ${base}{
    ######### BERT #########
    bert = bert-base-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 128             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/bert_base/
}


bert-large = ${base}{
    ######### BERT #########
    bert = bert-large-cased     # name of hugging face transformer model
    bert_emb_size = 1024        # name of hugging face transformer model
    segm_size = 384             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3            # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/bert_large
}


spanbert-base = ${base}{
    ######### BERT #########
    bert = SpanBERT/spanbert-base-cased         # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 384             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3            # maximal number of segments per document (applied only for training)
    lr_bert = 2e-05             # initial learning rate for fine-tuning bert
    lr_task = 1e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/spanbert_base
}


spanbert-large = ${base}{
    ######### BERT #########
    bert = SpanBERT/spanbert-large-cased        # name of hugging face transformer model
    bert_emb_size = 1024        # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3            # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 3e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/spanbert_large
}

roberta-base = ${base}{
    ######### BERT #########
    bert = roberta-base         # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-05             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/roberta_base
}

multilingual-bert-base = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    input_folder = ../Datasets/CorefUD-1.0-public/data/CorefUD_Polish-PCC/
    data_folder = ./data/data/multilingual_bert_base
}

xlm-roberta-base = ${base}{
    ######### BERT #########
    bert = xlm-roberta-base                     # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_Spanish-AnCora/
    data_folder = ./data/data/xlm_roberta_base
}

multilingual-bert-base-russian = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50          # save checkpoint after every 'n' epochs
    patience = 50 
    epochs = 500

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = russian-morph-full.csv

    ######### PATHS ########
    input_folder = ./datasets/Russian/Base
    data_folder = ./data/data/multilingual_bert_base/Base

    ######### OVERRIDE ##########
    language = ru_rucor-corefud
    train_data_file = train.ru_rucor-corefud.jsonlines             
    eval_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-dev.conllu
    eval_data_file = dev.ru_rucor-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-test.conllu
    test_data_file = test.ru_rucor-corefud.jsonlines
    predictions_path = ./predictions/og/exp5/russian_corefud.conllu
}

multilingual-bert-base-russian-100 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs
    patience = 50 
    epochs = 500

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = russian-morph-lr-100.csv

    ######### PATHS ########
    input_folder = ./datasets/Russian/LR/100
    data_folder = ./data/data/multilingual_bert_base/LR/100

    ######### OVERRIDE ##########
    language = ru_rucor-corefud
    train_data_file = train.ru_rucor-corefud.jsonlines              
    eval_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-dev.conllu
    eval_data_file = dev.ru_rucor-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-test.conllu
    test_data_file = test.ru_rucor-corefud.jsonlines
    predictions_path = ./predictions/lr/100/exp5/russian_corefud.conllu
}

multilingual-bert-base-russian-50 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs 
    patience = 100 
    epochs = 750

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = russian-morph-lr-50.csv

    ######### PATHS ########
    input_folder = ./datasets/Russian/LR/50
    data_folder = ./data/data/multilingual_bert_base/LR/50

    ######### OVERRIDE ##########
    language = ru_rucor-corefud
    train_data_file = train.ru_rucor-corefud.jsonlines           
    eval_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-dev.conllu
    eval_data_file = dev.ru_rucor-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-test.conllu
    test_data_file = test.ru_rucor-corefud.jsonlines
    predictions_path = ./predictions/lr/50/exp5/russian_corefud.conllu
}

multilingual-bert-base-russian-10 = ${base}{
     ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs
    patience = 150 
    epochs = 1000 

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = russian-morph-lr-10.csv

    ######### PATHS ########
    input_folder = ./datasets/Russian/LR/10
    data_folder = ./data/data/multilingual_bert_base/LR/10

    ######### OVERRIDE ##########
    language = ru_rucor-corefud
    train_data_file = train.ru_rucor-corefud.jsonlines           
    eval_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-dev.conllu
    eval_data_file = dev.ru_rucor-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-test.conllu
    test_data_file = test.ru_rucor-corefud.jsonlines
    predictions_path = ./predictions/lr/10/exp5/russian_corefud.conllu
}

multilingual-bert-base-polish = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs
    patience = 50 
    epochs = 500  

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = polish-baseline-full.csv

    ######### PATHS ########
    input_folder = ./datasets/Polish/Base
    data_folder = ./data/data/multilingual_bert_base/Base

    ######### OVERRIDE ##########
    language = pl_pcc-corefud
    train_data_file = train.pl_pcc-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/Polish/Base/pl_pcc-corefud-dev.conllu
    eval_data_file = dev.pl_pcc-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Polish/Base/pl_pcc-corefud-test.conllu
    test_data_file = test.pl_pcc-corefud.jsonlines
    predictions_path = ./predictions/og/exp5/polish_corefud.conllu
}

multilingual-bert-base-polish-100 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 
    patience = 50 
    epochs = 500  

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = polish-morph-lr-100.csv

    ######### PATHS ########
    input_folder = ./datasets/Polish/LR/100
    data_folder = ./data/data/multilingual_bert_base/LR/100

    ######### OVERRIDE ##########
    language = pl_pcc-corefud
    train_data_file = train.pl_pcc-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/Polish/Base/pl_pcc-corefud-dev.conllu
    eval_data_file = dev.pl_pcc-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Polish/Base/pl_pcc-corefud-test.conllu
    test_data_file = test.pl_pcc-corefud.jsonlines
    predictions_path = ./predictions/lr/100/exp5/polish_corefud.conllu
}

multilingual-bert-base-polish-50 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs
    patience = 100 
    epochs = 750   

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = polish-morph-lr-50.csv

    ######### PATHS ########
    input_folder = ./datasets/Polish/LR/50
    data_folder = ./data/data/multilingual_bert_base/LR/50

    ######### OVERRIDE ##########
    language = pl_pcc-corefud
    train_data_file = train.pl_pcc-corefud.jsonlines          
    eval_gold_corefud_path = ./datasets/Polish/Base/pl_pcc-corefud-dev.conllu
    eval_data_file = dev.pl_pcc-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Polish/Base/pl_pcc-corefud-test.conllu
    test_data_file = test.pl_pcc-corefud.jsonlines
    predictions_path = ./predictions/lr/50/exp5/polish_corefud.conllu
}

multilingual-bert-base-polish-10 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 
    patience = 150 
    epochs = 1000  

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = polish-morph-lr-10.csv

    ######### PATHS ########
    input_folder = ./datasets/Polish/LR/10
    data_folder = ./data/data/multilingual_bert_base/LR/10

    ######### OVERRIDE ##########
    language = pl_pcc-corefud
    train_data_file = train.pl_pcc-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/Polish/Base/pl_pcc-corefud-dev.conllu
    eval_data_file = dev.pl_pcc-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Polish/Base/pl_pcc-corefud-test.conllu
    test_data_file = test.pl_pcc-corefud.jsonlines
    predictions_path = ./predictions/lr/10/exp5/polish_corefud.conllu
}

multilingual-bert-base-hungarian = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs
    patience = 50 
    epochs = 500  

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = hungarian-morph-full.csv

    ######### PATHS ########
    input_folder = ./datasets/Hungarian/Base
    data_folder = ./data/data/multilingual_bert_base/Base

    ######### OVERRIDE ##########
    language = hu_szegedkoref-corefud
    train_data_file = train.hu_szegedkoref-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/Hungarian/Base/hu_szegedkoref-corefud-dev.conllu
    eval_data_file = dev.hu_szegedkoref-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Hungarian/Base/hu_szegedkoref-corefud-test.conllu
    test_data_file = test.hu_szegedkoref-corefud.jsonlines
    predictions_path = ./predictions/og/exp5/hungarian_corefud.conllu
}

multilingual-bert-base-hungarian-100 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 
    patience = 50 
    epochs = 500  

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = hungarian-morph-lr-100.csv

    ######### PATHS ########
    input_folder = ./datasets/Hungarian/LR/100
    data_folder = ./data/data/multilingual_bert_base/LR/100

    ######### OVERRIDE ##########
    language = hu_szegedkoref-corefud
    train_data_file = train.hu_szegedkoref-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/Hungarian/Base/hu_szegedkoref-corefud-dev.conllu
    eval_data_file = dev.hu_szegedkoref-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Hungarian/Base/hu_szegedkoref-corefud-test.conllu
    test_data_file = test.hu_szegedkoref-corefud.jsonlines
    predictions_path = ./predictions/lr/100/exp5/hungarian_corefud.conllu
}

multilingual-bert-base-hungarian-50 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs
    patience = 100 
    epochs = 750   

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = hungarian-morph-lr-50.csv

    ######### PATHS ########
    input_folder = ./datasets/Hungarian/LR/50
    data_folder = ./data/data/multilingual_bert_base/LR/50

    ######### OVERRIDE ##########
    language = hu_szegedkoref-corefud
    train_data_file = train.hu_szegedkoref-corefud.jsonlines          
    eval_gold_corefud_path = ./datasets/Hungarian/Base/hu_szegedkoref-corefud-dev.conllu
    eval_data_file = dev.hu_szegedkoref-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Hungarian/Base/hu_szegedkoref-corefud-test.conllu
    test_data_file = test.hu_szegedkoref-corefud.jsonlines
    predictions_path = ./predictions/lr/50/exp5/hungarian_corefud.conllu
}

multilingual-bert-base-hungarian-10 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 
    patience = 150 
    epochs = 1000  

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = hungarian-morph-lr-10.csv

    ######### PATHS ########
    input_folder = ./datasets/Hungarian/LR/10
    data_folder = ./data/data/multilingual_bert_base/LR/10

    ######### OVERRIDE ##########
    language = hu_szegedkoref-corefud
    train_data_file = train.hu_szegedkoref-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/Hungarian/Base/hu_szegedkoref-corefud-dev.conllu
    eval_data_file = dev.hu_szegedkoref-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Hungarian/Base/hu_szegedkoref-corefud-test.conllu
    test_data_file = test.hu_szegedkoref-corefud.jsonlines
    predictions_path = ./predictions/lr/10/exp5/hungarian_corefud.conllu
}

multilingual-bert-base-spanish = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 

    ######## LOGGING ########
    log_after = 100              # log after every 'n' batches

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_Spanish-AnCora/
    data_folder = ./data/data/multilingual_bert_base

    ######### OVERRIDE ##########
    language = es_ancora-corefud
    train_data_file = train.es_ancora-corefud.jsonlines
    epochs = 60               
    eval_gold_path = ./data/data/dev.es_ancora-corefud.v4_auto_conll
    eval_gold_coreud_path = ./data/data/es_ancora-corefud-dev.conllu
    eval_data_file = dev.es_ancora-corefud.jsonlines
    predictions_path = ./predictions/exp1/spanish_corefud.conllu
}

multilingual-bert-base-lithuanian = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs 

    ######## LOGGING ########
    log_after = 20              # log after every 'n' batches

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_Lithuanian-LCC/
    data_folder = ./data/data/multilingual_bert_base

    ######### OVERRIDE ##########
    language = lt_lcc-corefud
    train_data_file = train.lt_lcc-corefud.jsonlines
    epochs = 500               
    eval_gold_path = ./data/data/dev.lt_lcc-corefud.v4_auto_conll
    eval_gold_coreud_path = ./data/data/lt_lcc-corefud-dev.conllu
    eval_data_file = dev.lt_lcc-corefud.jsonlines
    predictions_path = ./predictions/exp1/lithuanian_corefud.conllu
}

multilingual-bert-base-french = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs 

    ######## LOGGING ########
    log_after = 20              # log after every 'n' batches

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_French-Democrat/
    data_folder = ./data/data/multilingual_bert_base

    ######### OVERRIDE ##########
    language = fr_democrat-corefud
    train_data_file = train.fr_democrat-corefud.jsonlines
    epochs = 500               
    eval_gold_path = ./data/data/dev.fr_democrat-corefud.v4_auto_conll
    eval_gold_coreud_path = ./data/data/fr_democrat-corefud-dev.conllu
    eval_data_file = dev.fr_democrat-corefud.jsonlines
    predictions_path = ./predictions/exp1/french_corefud.conllu
}

multilingual-bert-base-german = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs 

    ######## LOGGING ########
    log_after = 20              # log after every 'n' batches

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_German-PotsdamCC/
    data_folder = ./data/data/multilingual_bert_base

    ######### OVERRIDE ##########
    language = de_potsdamcc-corefud
    train_data_file = train.de_potsdamcc-corefud.jsonlines
    epochs = 500               
    eval_gold_path = ./data/data/dev.de_potsdamcc-corefud.v4_auto_conll
    eval_gold_coreud_path = ./data/data/de_potsdamcc-corefud-dev.conllu
    eval_data_file = dev.de_potsdamcc-corefud.jsonlines
    predictions_path = ./predictions/exp1/german_corefud.conllu
}