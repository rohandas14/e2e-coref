base {
    ########## DATA #########
    max_ment_width = 30         # maximal number of words in a single mention
    genres = [bc, bn, mz, nw, pt, tc, wb] # possible genres for a document

    ######### MODEL ########
    bin_widths = [1, 1, 1, 1, 1, 3, 8, 16, 32, 1]   # width of bins for mention distance embedding
    dropout = 0.3               # dropout probability for certain layers
    feature_size = 20           # embedding size for certain features e.g. genre, width etc.
    hidden_size = 3000          # hidden layer size of mention and antecedent scorers
    hidden_depth = 1            # number of hidden layers for several scorers
    max_ment_dist = 250         # maximal distance to antecedent (measured in words)
    ment_ratio = 0.4            # used to calculate top k mention to consider
    max_antes = 50              # max number of antecedents per mention to consider
    coref_depth = 2             # number of iterations of inference procedure
}

multilingual-bert-base = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_Polish-PCC/
    data_folder = ./data/data/multilingual_bert_base
}

multilingual-bert-base-russian = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50          # save checkpoint after every 'n' epochs
    patience = 50 
    epochs = 500

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = russian-baseline-full.csv

    ######### PATHS ########
    input_folder = ./datasets/Russian/Base
    data_folder = ./data/data/multilingual_bert_base/Random/Base

    ######### OVERRIDE ##########
    language = ru_rucor-corefud
    train_data_file = train.ru_rucor-corefud.jsonlines             
    eval_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-dev.conllu
    eval_data_file = dev.ru_rucor-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-test.conllu
    test_data_file = test.ru_rucor-corefud.jsonlines
    predictions_path = ./predictions/og/baseline/russian_corefud.conllu
}

multilingual-bert-base-russian-100 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs
    patience = 50 
    epochs = 500

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = russian-baseline-lr-100.csv

    ######### PATHS ########
    input_folder = ./datasets/Russian/LR/100
    data_folder = ./data/data/multilingual_bert_base/Random/LR/100

    ######### OVERRIDE ##########
    language = ru_rucor-corefud
    train_data_file = train.ru_rucor-corefud.jsonlines              
    eval_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-dev.conllu
    eval_data_file = dev.ru_rucor-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-test.conllu
    test_data_file = test.ru_rucor-corefud.jsonlines
    predictions_path = ./predictions/lr/100/baseline/russian_corefud.conllu
}

multilingual-bert-base-russian-50 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs 
    patience = 100 
    epochs = 750

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = russian-baseline-lr-50.csv

    ######### PATHS ########
    input_folder = ./datasets/Russian/LR/50
    data_folder = ./data/data/multilingual_bert_base/Random/LR/50

    ######### OVERRIDE ##########
    language = ru_rucor-corefud
    train_data_file = train.ru_rucor-corefud.jsonlines           
    eval_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-dev.conllu
    eval_data_file = dev.ru_rucor-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-test.conllu
    test_data_file = test.ru_rucor-corefud.jsonlines
    predictions_path = ./predictions/lr/50/baseline/russian_corefud.conllu
}

multilingual-bert-base-russian-10 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs
    patience = 150 
    epochs = 1000 

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = russian-baseline-lr-10.csv

    ######### PATHS ########
    input_folder = ./datasets/Russian/LR/10
    data_folder = ./data/data/multilingual_bert_base/Random/LR/10

    ######### OVERRIDE ##########
    language = ru_rucor-corefud
    train_data_file = train.ru_rucor-corefud.jsonlines           
    eval_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-dev.conllu
    eval_data_file = dev.ru_rucor-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Russian/Base/ru_rucor-corefud-test.conllu
    test_data_file = test.ru_rucor-corefud.jsonlines
    predictions_path = ./predictions/lr/10/baseline/russian_corefud.conllu
}

multilingual-bert-base-polish = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs
    patience = 25 
    epochs = 500  

    ######## LOGGING ########
    log_after = 100              # log after every 'n' batches
    log_file_name = polish-baseline-full.csv

    ######### PATHS ########
    input_folder = ./datasets/Polish/Base
    data_folder = ./data/data/multilingual_bert_base/Random/Base

    ######### OVERRIDE ##########
    language = pl_pcc-corefud
    train_data_file = train.pl_pcc-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/Polish/Random/Base/pl_pcc-corefud-dev.conllu
    eval_data_file = dev.pl_pcc-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Polish/Random/Base/pl_pcc-corefud-test.conllu
    test_data_file = test.pl_pcc-corefud.jsonlines
    predictions_path = ./predictions/og/baseline/polish_corefud.conllu
}

multilingual-bert-base-polish-100 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 
    patience = 50 
    epochs = 500  

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = polish-baseline-lr-100.csv

    ######### PATHS ########
    input_folder = ./datasets/Polish/LR/100
    data_folder = ./data/data/multilingual_bert_base/Random/LR/100

    ######### OVERRIDE ##########
    language = pl_pcc-corefud
    train_data_file = train.pl_pcc-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/Polish/Random/Base/pl_pcc-corefud-dev.conllu
    eval_data_file = dev.pl_pcc-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Polish/Random/Base/pl_pcc-corefud-test.conllu
    test_data_file = test.pl_pcc-corefud.jsonlines
    predictions_path = ./predictions/lr/100/baseline/polish_corefud.conllu
}

multilingual-bert-base-polish-50 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs
    patience = 100 
    epochs = 750   

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = polish-baseline-lr-50.csv

    ######### PATHS ########
    input_folder = ./datasets/Polish/LR/50
    data_folder = ./data/data/multilingual_bert_base/Random/LR/50

    ######### OVERRIDE ##########
    language = pl_pcc-corefud
    train_data_file = train.pl_pcc-corefud.jsonlines          
    eval_gold_corefud_path = ./datasets/Polish/Random/Base/pl_pcc-corefud-dev.conllu
    eval_data_file = dev.pl_pcc-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Polish/Random/Base/pl_pcc-corefud-test.conllu
    test_data_file = test.pl_pcc-corefud.jsonlines
    predictions_path = ./predictions/lr/50/baseline/polish_corefud.conllu
}

multilingual-bert-base-polish-10 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 
    patience = 150 
    epochs = 1000  

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = polish-baseline-lr-10.csv

    ######### PATHS ########
    input_folder = ./datasets/Polish/LR/10
    data_folder = ./data/data/multilingual_bert_base/Random/LR/10

    ######### OVERRIDE ##########
    language = pl_pcc-corefud
    train_data_file = train.pl_pcc-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/Polish/Random/Base/pl_pcc-corefud-dev.conllu
    eval_data_file = dev.pl_pcc-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Polish/Random/Base/pl_pcc-corefud-test.conllu
    test_data_file = test.pl_pcc-corefud.jsonlines
    predictions_path = ./predictions/lr/10/baseline/polish_corefud.conllu
}

multilingual-bert-base-hungarian = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs
    patience = 50 
    epochs = 500  

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = hungarian-baseline-full.csv

    ######### PATHS ########
    input_folder = ./datasets/Hungarian/Base
    data_folder = ./data/data/multilingual_bert_base/Random/Base

    ######### OVERRIDE ##########
    language = hu_szegedkoref-corefud
    train_data_file = train.hu_szegedkoref-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/Hungarian/Random/Base/hu_szegedkoref-corefud-dev.conllu
    eval_data_file = dev.hu_szegedkoref-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Hungarian/Random/Base/hu_szegedkoref-corefud-test.conllu
    test_data_file = test.hu_szegedkoref-corefud.jsonlines
    predictions_path = ./predictions/og/baseline/hungarian_corefud.conllu
}

multilingual-bert-base-hungarian-100 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 
    patience = 50 
    epochs = 500  

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = hungarian-baseline-lr-100.csv

    ######### PATHS ########
    input_folder = ./datasets/Hungarian/LR/100
    data_folder = ./data/data/multilingual_bert_base/Random/LR/100

    ######### OVERRIDE ##########
    language = hu_szegedkoref-corefud
    train_data_file = train.hu_szegedkoref-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/Hungarian/Random/Base/hu_szegedkoref-corefud-dev.conllu
    eval_data_file = dev.hu_szegedkoref-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Hungarian/Random/Base/hu_szegedkoref-corefud-test.conllu
    test_data_file = test.hu_szegedkoref-corefud.jsonlines
    predictions_path = ./predictions/lr/100/baseline/hungarian_corefud.conllu
}

multilingual-bert-base-hungarian-50 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs
    patience = 100 
    epochs = 750   

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = hungarian-baseline-lr-50.csv

    ######### PATHS ########
    input_folder = ./datasets/Hungarian/LR/50
    data_folder = ./data/data/multilingual_bert_base/Random/LR/50

    ######### OVERRIDE ##########
    language = hu_szegedkoref-corefud
    train_data_file = train.hu_szegedkoref-corefud.jsonlines          
    eval_gold_corefud_path = ./datasets/Hungarian/Random/Base/hu_szegedkoref-corefud-dev.conllu
    eval_data_file = dev.hu_szegedkoref-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Hungarian/Random/Base/hu_szegedkoref-corefud-test.conllu
    test_data_file = test.hu_szegedkoref-corefud.jsonlines
    predictions_path = ./predictions/lr/50/baseline/hungarian_corefud.conllu
}

multilingual-bert-base-hungarian-10 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 
    patience = 150 
    epochs = 1000  

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = hungarian-baseline-lr-10.csv

    ######### PATHS ########
    input_folder = ./datasets/Hungarian/LR/10
    data_folder = ./data/data/multilingual_bert_base/Random/LR/10

    ######### OVERRIDE ##########
    language = hu_szegedkoref-corefud
    train_data_file = train.hu_szegedkoref-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/Hungarian/Random/Base/hu_szegedkoref-corefud-dev.conllu
    eval_data_file = dev.hu_szegedkoref-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Hungarian/Random/Base/hu_szegedkoref-corefud-test.conllu
    test_data_file = test.hu_szegedkoref-corefud.jsonlines
    predictions_path = ./predictions/lr/10/baseline/hungarian_corefud.conllu
}

multilingual-bert-base-lithuanian = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50          # save checkpoint after every 'n' epochs
    patience = 50 
    epochs = 500

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = lithuanian-baseline-full.csv

    ######### PATHS ########
    input_folder = ./datasets/Lithuanian/Base
    data_folder = ./data/data/multilingual_bert_base/Random/Base

    ######### OVERRIDE ##########
    language = lt_lcc-corefud
    train_data_file = train.lt_lcc-corefud.jsonlines             
    eval_gold_corefud_path = ./datasets/Lithuanian/Random/Base/lt_lcc-corefud-dev.conllu
    eval_data_file = dev.lt_lcc-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Lithuanian/Random/Base/lt_lcc-corefud-test.conllu
    test_data_file = test.lt_lcc-corefud.jsonlines
    predictions_path = ./predictions/og/baseline/lithuanian.conllu
}

multilingual-bert-base-lithuanian-50 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs 
    patience = 100 
    epochs = 750

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = lithuanian-baseline-lr-50.csv

    ######### PATHS ########
    input_folder = ./datasets/Lithuanian/LR/50
    data_folder = ./data/data/multilingual_bert_base/Random/LR/50

    ######### OVERRIDE ##########
    language = lt_lcc-corefud
    train_data_file = train.lt_lcc-corefud.jsonlines           
    eval_gold_corefud_path = ./datasets/Lithuanian/Random/Base/lt_lcc-corefud-dev.conllu
    eval_data_file = dev.lt_lcc-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Lithuanian/Random/Base/lt_lcc-corefud-test.conllu
    test_data_file = test.lt_lcc-corefud.jsonlines
    predictions_path = ./predictions/lr/50/baseline/lithuanian_corefud.conllu
}

multilingual-bert-base-lithuanian-10 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs
    patience = 150 
    epochs = 1000 

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = lithuanian-baseline-lr-10.csv

    ######### PATHS ########
    input_folder = ./datasets/Lithuanian/LR/10
    data_folder = ./data/data/multilingual_bert_base/Random/LR/10

    ######### OVERRIDE ##########
    language = lt_lcc-corefud
    train_data_file = train.lt_lcc-corefud.jsonlines           
    eval_gold_corefud_path = ./datasets/Lithuanian/Random/Base/lt_lcc-corefud-dev.conllu
    eval_data_file = dev.lt_lcc-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Lithuanian/Random/Base/lt_lcc-corefud-test.conllu
    test_data_file = test.lt_lcc-corefud.jsonlines
    predictions_path = ./predictions/lr/10/baseline/lithuanian_corefud.conllu
}

multilingual-bert-base-german = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50          # save checkpoint after every 'n' epochs
    patience = 50 
    epochs = 500

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = german-baseline-full.csv

    ######### PATHS ########
    input_folder = ./datasets/German/Base
    data_folder = ./data/data/multilingual_bert_base/Random/Base

    ######### OVERRIDE ##########
    language = de_potsdamcc-corefud
    train_data_file = train.de_potsdamcc-corefud.jsonlines             
    eval_gold_corefud_path = ./datasets/German/Random/Base/de_potsdamcc-corefud-dev.conllu
    eval_data_file = dev.de_potsdamcc-corefud.jsonlines
    test_gold_corefud_path = ./datasets/German/Random/Base/de_potsdamcc-corefud-test.conllu
    test_data_file = test.de_potsdamcc-corefud.jsonlines
    predictions_path = ./predictions/og/baseline/german_corefud.conllu
}

multilingual-bert-base-german-100 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs
    patience = 50 
    epochs = 500

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = german-baseline-lr-100.csv

    ######### PATHS ########
    input_folder = ./datasets/German/LR/100
    data_folder = ./data/data/multilingual_bert_base/Random/LR/100

    ######### OVERRIDE ##########
    language = de_potsdamcc-corefud
    train_data_file = train.de_potsdamcc-corefud.jsonlines              
    eval_gold_corefud_path = ./datasets/German/Random/Base/de_potsdamcc-corefud-dev.conllu
    eval_data_file = dev.de_potsdamcc-corefud.jsonlines
    test_gold_corefud_path = ./datasets/German/Random/Base/de_potsdamcc-corefud-test.conllu
    test_data_file = test.de_potsdamcc-corefud.jsonlines
    predictions_path = ./predictions/lr/100/baseline/german_corefud.conllu
}

multilingual-bert-base-german-50 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs 
    patience = 100 
    epochs = 750

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = german-baseline-lr-50.csv

    ######### PATHS ########
    input_folder = ./datasets/German/LR/50
    data_folder = ./data/data/multilingual_bert_base/Random/LR/50

    ######### OVERRIDE ##########
    language = de_potsdamcc-corefud
    train_data_file = train.de_potsdamcc-corefud.jsonlines           
    eval_gold_corefud_path = ./datasets/German/Random/Base/de_potsdamcc-corefud-dev.conllu
    eval_data_file = dev.de_potsdamcc-corefud.jsonlines
    test_gold_corefud_path = ./datasets/German/Random/Base/de_potsdamcc-corefud-test.conllu
    test_data_file = test.de_potsdamcc-corefud.jsonlines
    predictions_path = ./predictions/lr/50/baseline/german_corefud.conllu
}

multilingual-bert-base-german-10 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs
    patience = 150 
    epochs = 1000 

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = german-baseline-lr-10.csv

    ######### PATHS ########
    input_folder = ./datasets/German/LR/10
    data_folder = ./data/data/multilingual_bert_base/LR/10

    ######### OVERRIDE ##########
    language = de_potsdamcc-corefud
    train_data_file = train.de_potsdamcc-corefud.jsonlines           
    eval_gold_corefud_path = ./datasets/German/Random/Base/de_potsdamcc-corefud-dev.conllu
    eval_data_file = dev.de_potsdamcc-corefud.jsonlines
    test_gold_corefud_path = ./datasets/German/Random/Base/de_potsdamcc-corefud-test.conllu
    test_data_file = test.de_potsdamcc-corefud.jsonlines
    predictions_path = ./predictions/lr/10/baseline/german_corefud.conllu
}

multilingual-bert-base-czech = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs
    patience = 25 
    epochs = 500  

    ######## LOGGING ########
    log_after = 100              # log after every 'n' batches
    log_file_name = czech-baseline-full.csv

    ######### PATHS ########
    input_folder = ./datasets/Czech/Base
    data_folder = ./data/data/multilingual_bert_base/Base

    ######### OVERRIDE ##########
    language = cs_pcedt-corefud
    train_data_file = train.cs_pcedt-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/Czech/Base/cs_pcedt-corefud-dev.conllu
    eval_data_file = dev.cs_pcedt-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Czech/Base/cs_pcedt-corefud-test.conllu
    test_data_file = test.cs_pcedt-corefud.jsonlines
    predictions_path = ./predictions/og/baseline/czech_corefud.conllu
}

multilingual-bert-base-czech-100 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 
    patience = 50 
    epochs = 500  

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = czech-baseline-lr-100.csv

    ######### PATHS ########
    input_folder = ./datasets/Czech/LR/100
    data_folder = ./data/data/multilingual_bert_base/LR/100

    ######### OVERRIDE ##########
    language = cs_pcedt-corefud
    train_data_file = train.cs_pcedt-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/Czech/Base/cs_pcedt-corefud-dev.conllu
    eval_data_file = dev.cs_pcedt-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Czech/Base/cs_pcedt-corefud-test.conllu
    test_data_file = test.cs_pcedt-corefud.jsonlines
    predictions_path = ./predictions/lr/100/baseline/czech_corefud.conllu
}

multilingual-bert-base-czech-50 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs
    patience = 100 
    epochs = 750   

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = czech-baseline-lr-50.csv

    ######### PATHS ########
    input_folder = ./datasets/Czech/LR/50
    data_folder = ./data/data/multilingual_bert_base/LR/50

    ######### OVERRIDE ##########
    language = cs_pcedt-corefud
    train_data_file = train.cs_pcedt-corefud.jsonlines          
    eval_gold_corefud_path = ./datasets/Czech/Base/cs_pcedt-corefud-dev.conllu
    eval_data_file = dev.cs_pcedt-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Czech/Base/cs_pcedt-corefud-test.conllu
    test_data_file = test.cs_pcedt-corefud.jsonlines
    predictions_path = ./predictions/lr/50/baseline/czech_corefud.conllu
}

multilingual-bert-base-czech-10 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 
    patience = 150 
    epochs = 1000  

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = czech-baseline-lr-10.csv

    ######### PATHS ########
    input_folder = ./datasets/Czech/LR/10
    data_folder = ./data/data/multilingual_bert_base/LR/10

    ######### OVERRIDE ##########
    language = cs_pcedt-corefud
    train_data_file = train.cs_pcedt-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/Czech/Base/cs_pcedt-corefud-dev.conllu
    eval_data_file = dev.cs_pcedt-corefud.jsonlines
    test_gold_corefud_path = ./datasets/Czech/Base/cs_pcedt-corefud-test.conllu
    test_data_file = test.cs_pcedt-corefud.jsonlines
    predictions_path = ./predictions/lr/10/baseline/czech_corefud.conllu
}

multilingual-bert-base-french-10 = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 
    patience = 150 
    epochs = 1000  

    ######## LOGGING ########
    log_after = 10              # log after every 'n' batches
    log_file_name = french-baseline-lr-10.csv

    ######### PATHS ########
    input_folder = ./datasets/French/LR/10
    data_folder = ./data/data/multilingual_bert_base/LR/10

    ######### OVERRIDE ##########
    language = fr_democrat-corefud
    train_data_file = train.fr_democrat-corefud.jsonlines
    eval_gold_corefud_path = ./datasets/French/Base/fr_democrat-corefud-dev.conllu
    eval_data_file = dev.fr_democrat-corefud.jsonlines
    test_gold_corefud_path = ./datasets/French/Base/fr_democrat-corefud-test.conllu
    test_data_file = test.fr_democrat-corefud.jsonlines
    predictions_path = ./predictions/lr/10/baseline/french_corefud.conllu
}

multilingual-bert-base-spanish = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 

    ######## LOGGING ########
    log_after = 100              # log after every 'n' batches

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_Spanish-AnCora/
    data_folder = ./data/data/multilingual_bert_base

    ######### OVERRIDE ##########
    language = es_ancora-corefud
    train_data_file = train.es_ancora-corefud.jsonlines
    epochs = 60               
    eval_gold_path = ./data/data/dev.es_ancora-corefud.v4_auto_conll
    eval_gold_coreud_path = ./data/data/es_ancora-corefud-dev.conllu
    eval_data_file = dev.es_ancora-corefud.jsonlines
    predictions_path = ./predictions/baseline/spanish_corefud.conllu
}

multilingual-bert-base-catalan = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 5           # save checkpoint after every 'n' epochs 

    ######## LOGGING ########
    log_after = 100              # log after every 'n' batches

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_Catalan-AnCora/
    data_folder = ./data/data/multilingual_bert_base

    ######### OVERRIDE ##########
    language = ca_ancora-corefud
    train_data_file = train.ca_ancora-corefud.jsonlines
    epochs = 60               
    eval_gold_path = ./data/data/dev.ca_ancora-corefud.v4_auto_conll
    eval_gold_coreud_path = ./data/data/ca_ancora-corefud-dev.conllu
    eval_data_file = dev.ca_ancora-corefud.jsonlines
    predictions_path = ./predictions/baseline/catalan_corefud.conllu
}

multilingual-bert-base-french = ${base}{
    ######### BERT #########
    bert = bert-base-multilingual-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers
    ckpt_interval = 50           # save checkpoint after every 'n' epochs 

    ######## LOGGING ########
    log_after = 20              # log after every 'n' batches

    ######### PATHS ########
    input_folder = ./CorefUD-1.0-public/data/CorefUD_French-Democrat/
    data_folder = ./data/data/multilingual_bert_base

    ######### OVERRIDE ##########
    language = fr_democrat-corefud
    train_data_file = train.fr_democrat-corefud.jsonlines
    epochs = 500               
    eval_gold_path = ./data/data/dev.fr_democrat-corefud.v4_auto_conll
    eval_gold_coreud_path = ./data/data/fr_democrat-corefud-dev.conllu
    eval_data_file = dev.fr_democrat-corefud.jsonlines
    predictions_path = ./predictions/baseline/french_corefud.conllu
}